## This is the repository for "千问LLM"
### 欢迎大家关注！并且提需要分享的内容，我这边准备。 
### 不更新到千问 绝不罢休！
#### [千问LLM之一：Batch输入训练或者推理为何需要padding？](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483926&idx=1&sn=7e324b574506f7ee4044c20b62906a61&chksm=eb552315dc22aa032632689a782db6dbac01a36df5eeb5de07871ce8bebc73ddbe9b37817870&token=1316523041&lang=zh_CN#rd) 
#### [千问LLM之二：LLM tokenizer 是怎么做的？](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483929&idx=1&sn=bf6e207a0a4e81cd749c071ce1d0cf17&chksm=eb55231adc22aa0c2959190b8c4a0ea69798c7d3bf7fce3b994599bed538383d5e282b1f0cdc&token=1316523041&lang=zh_CN#rd)

#### [千问LLM之三：LLM Embedding是怎么做的？ ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483926&idx=1&sn=7e324b574506f7ee4044c20b62906a61&chksm=eb552315dc22aa032632689a782db6dbac01a36df5eeb5de07871ce8bebc73ddbe9b37817870&token=1316523041&lang=zh_CN#rd)

#### [千问LLM之四：LLM Transformer 架构：Encoder](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483943&idx=1&sn=60da33acc8a3499d74d24e891e5d955f&chksm=eb552324dc22aa329bc6f9fce1b241b1d08412418e3930aec17d854e1409e03b2c3fff6f53b6&token=1316523041&lang=zh_CN#rd)

#### [千问LLM之五：LLM Transformer 架构：Decoder ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483943&idx=2&sn=f689e3ff6cc400971b275e8c93c6baba&chksm=eb552324dc22aa32ca5f03f86440b58eaecf96ac2875975e5849df50955cc0657bd23b0bfbf2&token=1316523041&lang=zh_CN#rd)
#### [千问LLM之六：LLM 生成式 GPT  ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483957&idx=1&sn=0313d3316914a93c8bcb6ef2b653cf77&chksm=eb552336dc22aa20e482c6fc0f7a8a81c248e45a1accc7202295748654500beef974fe86ac0c&token=1127561406&lang=zh_CN&poc_token=HC_9OWejuFDn9f8xKKcjUyi8R2xJ0rxsKpdzt9fe)

#### [千问LLM之七：LLM 之 BERT ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483957&idx=2&sn=f0ee665c58fe580edd77caacca15546e&chksm=eb552336dc22aa2002ca3354b75037051de854ac571f62caaf434fd0bb9c328c2369e3de79ed&token=1127561406&lang=zh_CN#rd)
#### [千问LLM之八：LLM 之 AutoRegressive and MLM ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483964&idx=1&sn=64cc26bdc6c215535fe2486f2fb27b8c&chksm=eb55233fdc22aa29d4538b1eb16b2ff04fe4a3c12f7c2209d54e50537269427babc643486335&token=1127561406&lang=zh_CN#rd)
#### [千问LLM之九：LLM 之 幻觉 hallucination ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483964&idx=2&sn=84037d171d7a842d7afd0e82f7475568&chksm=eb55233fdc22aa29f656085ae26b3494566ede8f4817f3ecabc88e550da4596b270121cad1a6&token=1127561406&lang=zh_CN#rd)
#### [千问LLM之十：Transformer 解决了RNN的哪些问题？](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483964&idx=3&sn=464de4c2071e12ab41b8019ee48b84ea&chksm=eb55233fdc22aa29fa82f7ade31b13e007f82379e706925d132a84aff372ebfbc58ed2f6d3fa&token=1127561406&lang=zh_CN#rd)
#### [千问LLM之十一：LLM之什么是 Diffusion Model 扩散模型？](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483964&idx=4&sn=78b8fc9d967f2f8b1cf2a11cb146c4c2&chksm=eb55233fdc22aa29f3ba0736a68a8477976f9d94291261e382f8418c84e8d5a2c21451e80b73&token=1127561406&lang=zh_CN#rd)
#### [千问LLM之十二：什么是梯度爆炸？](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483964&idx=5&sn=c4159126bb6eda148d135b3f20411b4c&chksm=eb55233fdc22aa29f80739780a32837f3e136dcd745e74c49763e4bc4c0eee1d49223120bc7f&token=1127561406&lang=zh_CN#rd)
#### [千问LLM之十三：Sequence 2 Sequence 模型和Transformer模型有什么根本差别？ ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483974&idx=1&sn=e100a0bd03ffffe145eddb9f1122ca29&chksm=eb552345dc22aa53d09b888a78cccab720a91e6a5e5bb541f7e1aad96e0668ebb8a2033cde3f&token=246431861&lang=zh_CN#rd)
#### [千问LLM之十四：什么是LoRA ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483969&idx=1&sn=374dda0ab4c1c6d0b142e4ae8dc0b038&chksm=eb552342dc22aa54cec97b2bc9eedb0faa7520d1b8beb516f33046ef9e425e45c822697f50e6&token=1127561406&lang=zh_CN#rd)

#### [千问LLM之十五：什么是QLoRA？ ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483982&idx=1&sn=a319867c62f1d0a81a62f8755f232e29&chksm=eb55234ddc22aa5bb72c953a3101f5ca3f4906359f1c18840ae9a175f8756f209afd520b29d9&token=246431861&lang=zh_CN#rd)

#### [千问LLM之十六：开源模型文件里面通常包含哪些内容？可以逆向推出模型的训练脚本吗？ ](https://mp.weixin.qq.com/s?__biz=MzI3ODYwNzk4MA==&mid=2247483982&idx=2&sn=28bfed8b451f77c264197ffcd392bac7&chksm=eb55234ddc22aa5ba830ea83f86c02b8d11ceefe98c04fba60e021a7f9ad3a72cc25ad352a0f&token=246431861&lang=zh_CN#rd)

#### [千问LLM之十七：什么是 word2vector ? ](https://mp.weixin.qq.com/s/z1R_R4PsV0GqCgA8rCR8kA?token=332088811&lang=zh_CN)


#### [千问LLM之十八：何谓损失函数？如何设计损失函数？LLM 损失函数长啥样？ ](https://mp.weixin.qq.com/s/wOx_4HBe52Mply56HmLHRw?token=332088811&lang=zh_CN)


#### [ 千问LLM之十九：为啥GPT-4 Tokenize 中文的某些词语为 UNK 但是它的中文输出却感觉挺好的？ ](https://mp.weixin.qq.com/s/rfE-hkcAWB9VOWL74Un7Sg?token=332088811&lang=zh_CN)

#### [千问LLM之二十：什么是CNN卷积模型？](https://mp.weixin.qq.com/s/ozjqB0qixntbpqdB7vP7Ug?token=332088811&lang=zh_CN)

#### [千问LLM之二十一：什么是MultiModal多模态模型？ ](https://mp.weixin.qq.com/s/b9h2HtpcdzDx5Vtt49ljcQ?token=332088811&lang=zh_CN)

#### [千问LLM之二十二：到底谁才是LLM训练的真正幕后英雄？ ](https://mp.weixin.qq.com/s/fIO-EXAqM4EH6hC4mbhN_Q?token=332088811&lang=zh_CN)


#### [千问LLM之二十三：自动微分：让梯度跳起优雅的广场舞  ](https://mp.weixin.qq.com/s/kiY7u52Af1Ym69nt8jNgEA?token=853525989&lang=zh_CN)


#### [千问LLM之二十四：什么是UNET？](https://mp.weixin.qq.com/s/w3OZkQkejXiqPdFQQsn09w?token=853525989&lang=zh_CN)


#### [千问LLM之二十五：你猜AI界的“破案”扛把子会是谁？ ](https://mp.weixin.qq.com/s/2lF9grLbmdPaRO_g7Av4zQ?token=853525989&lang=zh_CN)

#### [千千问LLM之二十六：“破案”扛把子是如何训练出来的？从所犯的错误中学习。 ](https://mp.weixin.qq.com/s/B3O1CZGRPEvmytia6OUYdg?token=853525989&lang=zh_CN)

#### [千问LLM之二十七：三大AI助手的“奇葩说”：ChatGPT、KiMi和Claude的对决 ](https://mp.weixin.qq.com/s/9MkqzTRUJdkYC5Qlymug6w?token=853525989&lang=zh_CN)


#### [千问LLM之二十八：什么是Batch Normalization？什么又是Layer Normalization？](https://mp.weixin.qq.com/s/IQbd5dirFaSqd7ZKgZkyIQ?token=853525989&lang=zh_CN)

#### [千问LLM之二十九：什么是Pre-LayerNorm 和 Post-LayerNorm ？ ](https://mp.weixin.qq.com/s/Zf8WkxMNwQf7RfV1Zt_YJA?token=853525989&lang=zh_CN)

#### [千问LLM之三十：什么是Postion Encoding? ](https://mp.weixin.qq.com/s/ToE5cJ9wMSXbvZImHmcJSA?token=366761013&lang=zh_CN)

#### [千问LLM之三十一：LLM的解码都有哪些方式?  ](https://mp.weixin.qq.com/s/GwLd9TDoYveL0rCgGRM5fg?token=366761013&lang=zh_CN)

#### [千问LLM之三十二：AI界的“节食”计划 ](https://mp.weixin.qq.com/s/kZ0amTw0yFp388AmQ_9dVg)


#### [千问LLM之三十三：什么是 Sharding? 之数据并行（Data Parallelism）](https://mp.weixin.qq.com/s/jaG_P2WMMOKWGrAs3kRyMg?token=366761013&lang=zh_CN)

#### [千问LLM之三十四：什么是 Sharding? 之模型并行（Model Parallelism） ](https://mp.weixin.qq.com/s/S_-oTrgt3L0FWOgdOgbnsw?token=366761013&lang=zh_CN)


#### [千问LLM之三十五：什么是 Sharding? 之ZeRO 优化（Zero Redundancy Optimizer） ](https://mp.weixin.qq.com/s/okUpNxiy1hrK1qX_GoIc0g?token=366761013&lang=zh_CN)

#### [千问LLM之三十六：LLM的特工行动：工具召唤功能实战案例？Agent 到底是什么？ ](https://mp.weixin.qq.com/s/nWN3ExnpT3Ru4Os7_TMVPg?token=366761013&lang=zh_CN)